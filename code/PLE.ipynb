{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88fa037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "train = joblib.load('./data_and_feature/train.txt')\n",
    "val = joblib.load('./data_and_feature/val.txt')\n",
    "test = joblib.load('./data_and_feature/test.txt')\n",
    "encoder = joblib.load('./data_and_feature/encoder.txt')\n",
    "\n",
    "train_num = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10d21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "import os\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "from tensorflow.keras import optimizers,initializers\n",
    "from tensorflow.python.keras.initializers import glorot_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f41299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolLayer(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super(MeanPoolLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        mask = tf.expand_dims(tf.cast(mask,tf.float32),axis = -1)\n",
    "        x = x * mask\n",
    "        return K.sum(x, axis=self.axis) / (K.sum(mask, axis=self.axis) + 1e-9)\n",
    "\n",
    "class PleLayer(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    n_experts:list,每个任务使用几个expert。[2,3]第一个任务使用2个expert，第二个任务使用3个expert。\n",
    "    n_expert_share:int,共享的部分设置的expert个数。\n",
    "    expert_dim:int,每个专家网络输出的向量维度。\n",
    "    n_task:int,任务个数。\n",
    "    '''\n",
    "    def __init__(self,n_task,n_experts,expert_dim,n_expert_share,dnn_reg_l2 = 1e-5):\n",
    "        super(PleLayer, self).__init__()\n",
    "        self.n_task = n_task\n",
    "        \n",
    "        # 生成多个任务特定网络和1个共享网络。\n",
    "        self.E_layer = []\n",
    "        for i in range(n_task):\n",
    "            sub_exp = [Dense(expert_dim,activation = 'relu') for j in range(n_experts[i])]\n",
    "            self.E_layer.append(sub_exp)\n",
    "            \n",
    "        self.share_layer = [Dense(expert_dim,activation = 'relu') for j in range(n_expert_share)]\n",
    "        #定义门控网络\n",
    "        self.gate_layers = [Dense(n_expert_share+n_experts[i],kernel_regularizer=regularizers.l2(dnn_reg_l2),\n",
    "                                  activation = 'softmax') for i in range(n_task)]\n",
    "\n",
    "    def call(self,x):\n",
    "        #特定网络和共享网络\n",
    "        E_net = [[expert(x) for expert in sub_expert] for sub_expert in self.E_layer]\n",
    "        share_net = [expert(x) for expert in self.share_layer]\n",
    "        \n",
    "        #门的权重乘上，指定任务和共享任务的输出。\n",
    "        towers = []\n",
    "        for i in range(self.n_task):\n",
    "            g = self.gate_layers[i](x)\n",
    "            g = tf.expand_dims(g,axis = -1) #(bs,n_expert_share+n_experts[i],1)\n",
    "            _e = share_net+E_net[i]  \n",
    "            _e = Concatenate(axis = 1)([expert[:,tf.newaxis,:] for expert in _e]) #(bs,n_expert_share+n_experts[i],expert_dim)\n",
    "            _tower = tf.matmul(_e, g,transpose_a=True)\n",
    "            towers.append(Flatten()(_tower)) #(bs,expert_dim)\n",
    "        return towers\n",
    "\n",
    "def build_ple(sparse_cols,dense_cols,sparse_max_len,embed_dim,expert_dim = 4,\n",
    "              varlens_cols = [],varlens_max_len = [],dnn_hidden_units = (64,64),\n",
    "              n_task = 2,n_experts = [2,2],n_expert_share = 4,dnn_reg_l2 = 1e-6,\n",
    "              drop_rate = 0.0,embedding_reg_l2 = 1e-6,targets = []):\n",
    "\n",
    "   #输入部分，分为sparse,varlens,dense部分。\n",
    "    sparse_inputs = {f:Input([1],name = f) for f in sparse_cols}\n",
    "    dense_inputs = {f:Input([1],name = f) for f in dense_cols}\n",
    "    varlens_inputs = {f:Input([None,1],name = f) for f in varlens_cols}\n",
    "        \n",
    "    input_embed = {}\n",
    "    #离散特征，embedding到k维\n",
    "    for f in sparse_cols:\n",
    "        _input = sparse_inputs[f]\n",
    "        embedding = Embedding(sparse_max_len[f], embed_dim, \n",
    "            embeddings_regularizer=tf.keras.regularizers.l2(embedding_reg_l2)) \n",
    "        input_embed[f] =Flatten()(embedding(_input)) #(bs,k)\n",
    "        \n",
    "    #多标签离散变量\n",
    "    for f in varlens_inputs:\n",
    "        _input = varlens_inputs[f]\n",
    "        mask = Masking(mask_value = 0).compute_mask(_input)\n",
    "        embedding = Embedding(varlens_max_len[f], embed_dim,\n",
    "            embeddings_regularizer=tf.keras.regularizers.l2(1e-6))\n",
    "        _embed =Reshape([-1,embed_dim])(embedding(_input))\n",
    "        out_embed = MeanPoolLayer(axis=1)(_embed,mask)\n",
    "        input_embed[f] = out_embed\n",
    "        \n",
    "    input_embed.update(dense_inputs) #加入连续变量\n",
    "    input_embed = Concatenate(axis = -1)([input_embed[f] for f in input_embed])    \n",
    "                                  \n",
    "    for num in dnn_hidden_units:\n",
    "        input_embed = Dropout(drop_rate)(Dense(num,activation = 'relu',\n",
    "                    kernel_regularizer=regularizers.l2(dnn_reg_l2))(input_embed))\n",
    "    #Ple网络层\n",
    "    towers = PleLayer(n_task,n_experts,expert_dim,n_expert_share)(input_embed)\n",
    "    outputs = [Dense(1,activation = 'sigmoid',kernel_regularizer=regularizers.l2(dnn_reg_l2),\n",
    "                       name = f,use_bias = True)(_t) for f,_t in zip(targets,towers)]\n",
    "    inputs = [sparse_inputs[f] for f in sparse_inputs]+[varlens_inputs[f] for f in varlens_inputs]\\\n",
    "                +[dense_inputs[f] for f in dense_inputs]\n",
    "    model = Model(inputs,outputs) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56cabfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "656/656 [==============================] - 58s 82ms/step - loss: 0.2702 - read_comment_loss: 0.1006 - like_loss: 0.1023 - click_avatar_loss: 0.0426 - forward_loss: 0.0274 - read_comment_auc: 0.9105 - like_auc_1: 0.8177 - click_avatar_auc_2: 0.7795 - forward_auc_3: 0.7662 - val_loss: 0.2401 - val_read_comment_loss: 0.0934 - val_like_loss: 0.0897 - val_click_avatar_loss: 0.0369 - val_forward_loss: 0.0186 - val_read_comment_auc: 0.9197 - val_like_auc_1: 0.8316 - val_click_avatar_auc_2: 0.8030 - val_forward_auc_3: 0.7994\n",
      "Epoch 2/4\n",
      "656/656 [==============================] - 131s 200ms/step - loss: 0.2362 - read_comment_loss: 0.0901 - like_loss: 0.0893 - click_avatar_loss: 0.0351 - forward_loss: 0.0194 - read_comment_auc: 0.9353 - like_auc_1: 0.8623 - click_avatar_auc_2: 0.8481 - forward_auc_3: 0.8541 - val_loss: 0.2384 - val_read_comment_loss: 0.0924 - val_like_loss: 0.0891 - val_click_avatar_loss: 0.0363 - val_forward_loss: 0.0184 - val_read_comment_auc: 0.9251 - val_like_auc_1: 0.8359 - val_click_avatar_auc_2: 0.8196 - val_forward_auc_3: 0.8206\n",
      "Epoch 3/4\n",
      "656/656 [==============================] - 120s 183ms/step - loss: 0.2344 - read_comment_loss: 0.0891 - like_loss: 0.0887 - click_avatar_loss: 0.0345 - forward_loss: 0.0190 - read_comment_auc: 0.9379 - like_auc_1: 0.8653 - click_avatar_auc_2: 0.8580 - forward_auc_3: 0.8666 - val_loss: 0.2392 - val_read_comment_loss: 0.0929 - val_like_loss: 0.0890 - val_click_avatar_loss: 0.0359 - val_forward_loss: 0.0185 - val_read_comment_auc: 0.9220 - val_like_auc_1: 0.8362 - val_click_avatar_auc_2: 0.8246 - val_forward_auc_3: 0.8146\n",
      "Epoch 4/4\n",
      "656/656 [==============================] - 43s 66ms/step - loss: 0.2335 - read_comment_loss: 0.0884 - like_loss: 0.0883 - click_avatar_loss: 0.0341 - forward_loss: 0.0188 - read_comment_auc: 0.9392 - like_auc_1: 0.8675 - click_avatar_auc_2: 0.8634 - forward_auc_3: 0.8732 - val_loss: 0.2390 - val_read_comment_loss: 0.0922 - val_like_loss: 0.0890 - val_click_avatar_loss: 0.0362 - val_forward_loss: 0.0183 - val_read_comment_auc: 0.9234 - val_like_auc_1: 0.8363 - val_click_avatar_auc_2: 0.8315 - val_forward_auc_3: 0.8234\n"
     ]
    }
   ],
   "source": [
    "target = [\"read_comment\", \"like\", \"click_avatar\", \"forward\"]\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id']\n",
    "varlen_features = ['manual_tag_list','manual_keyword_list']\n",
    "dense_features = ['videoplayseconds']\n",
    "\n",
    "# 生成输入特征设置\n",
    "sparse_max_len = {f:len(encoder[f]) + 1 for f in sparse_features}\n",
    "varlens_max_len = {f:len(encoder[f]) + 1 for f in varlen_features}\n",
    "feature_names = sparse_features+varlen_features+dense_features\n",
    "\n",
    "# 构建输入数据\n",
    "train_model_input = {name: train[name] if name not in varlen_features else np.stack(train[name]) for name in feature_names } #训练模型的输入，字典类型。名称和具体值\n",
    "val_model_input = {name: val[name] if name not in varlen_features else np.stack(val[name]) for name in feature_names }\n",
    "test_model_input = {name: test[name] if name not in varlen_features else np.stack(test[name]) for name in feature_names}\n",
    "\n",
    "train_labels = [train[y].values for y in target]\n",
    "val_labels = [val[y].values for y in target]\n",
    "\n",
    "# 删除多余的数据，释放内存\n",
    "del train,val\n",
    "gc.collect()\n",
    "\n",
    "# 构建模型，训练和评估\n",
    "model = build_ple(sparse_features,dense_features,sparse_max_len,embed_dim = 16,expert_dim = 16,\n",
    "          varlens_cols = varlen_features,varlens_max_len = varlens_max_len,dnn_hidden_units = (64,),\n",
    "          n_task = 4,n_experts = [4,4,4,4],n_expert_share = 8,dnn_reg_l2 = 1e-6,\n",
    "          drop_rate = 0.1,embedding_reg_l2 = 1e-6,targets = target)\n",
    "\n",
    "adam = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# model.compile(adam, loss = 'binary_crossentropy' ,metrics = [tf.keras.metrics.AUC()],)\n",
    "model.compile(\n",
    "    adam, \n",
    "    loss={\n",
    "        'read_comment': 'binary_crossentropy',\n",
    "        'like': 'binary_crossentropy',\n",
    "        'click_avatar': 'binary_crossentropy',\n",
    "        'forward': 'binary_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'read_comment': 1.0,\n",
    "        'like': 0.8,\n",
    "        'click_avatar': 1.2,\n",
    "        'forward': 0.9,\n",
    "    },\n",
    "    metrics={\n",
    "        'read_comment': tf.keras.metrics.AUC(),\n",
    "        'like': tf.keras.metrics.AUC(),\n",
    "        'click_avatar': tf.keras.metrics.AUC(),\n",
    "        'forward': tf.keras.metrics.AUC(),\n",
    "    }\n",
    ")\n",
    "\n",
    "history = model.fit(train_model_input, train_labels,validation_data = (val_model_input,val_labels),\n",
    "                    batch_size=10240, epochs=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a357ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
